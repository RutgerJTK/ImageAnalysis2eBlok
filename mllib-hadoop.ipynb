{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
    "!tar -xvf spark-3.1.1-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-2-a312a4981562>, line 2)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-a312a4981562>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    os.environ[“JAVA_HOME”] = “/usr/lib/jvm/java-11-openjdk-amd64”\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from google.colab import files\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import isnan, when, count, col, lit\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd \n",
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "from pyspark.sql import SparkSession\n",
    "# create sparksession\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pysparkexample\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.upload()\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= sc.read.csv(\"data.csv\", inferSchema=True, header=True)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(column, value):\n",
    "    return when(column != value, column).otherwise(lit(None))\n",
    "data = data.withColumn(\"Market Category\", replace(col(\"Market Category\"), \"N/A\"))\n",
    "data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).show()\n",
    "data = data.drop(\"Market Category\")\n",
    "data = data.na.drop()\n",
    "print((data.count(), len(data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"Year\",\n",
    "                                      \"Engine HP\",\n",
    "                                      \"Engine Cylinders\",\n",
    "                                      \"Number of Doors\",\n",
    "                                      \"highway MPG\",\n",
    "                                      \"city mpg\",\n",
    "                                      \"Popularity\"], outputCol = \"Attributes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(featuresCol = \"Attributes\", labelCol=\"MSR\")\n",
    "pipeline = Pipeline(stages=[assembler, regressor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.write().overwrite().save(\"pipeline\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "N6ZDpd9XzFeN"
   ],
   "name": "Text classification with TF-Hub",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python387jvsc74a57bd0d6121a2be9be14c350818965fb41ffcfa13bfc075c0f01a5a51b266cf1a319be",
   "display_name": "Python 3.8.7 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "d6121a2be9be14c350818965fb41ffcfa13bfc075c0f01a5a51b266cf1a319be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}